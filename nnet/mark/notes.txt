
read until May 9th: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/12882/guess-this-is-really-the-time-to-try-out-neural-nets?page=5
read until May 9th: feature: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14092/feature-generation-for-dataset-ideas
read until May 9th: NN https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13632/achieve-0-48-in-5-min-with-a-deep-net-feat-batchnorm-prelu
read until May 9th: difficult cls: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13768/how-to-classify-the-class-2-3-4-better
long tutorial: http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/


#todo: investigate overfitting better!
#todo: predictions from several nets into one

naming scheme for files etc is a mess

Maybe make several NNs and feed the probabilities into a new NN
The key point of my method is that I tuned NN hyper parameters for an averaging model rather than for a single model.
BatchNorm and PReLU

Kinda done:
- An overfitting net can generally be made to perform better by using more training data.
- If your unregularized net does not overfit, you should probably make it larger.
- If I can store just the weights and biasses, I can start the network from those points instead of reloading everything.
- Dealing with class imbalance: undersampling, class weight, and rescaling probabilities
-

Maybe later:
- Really hard time to distinguish classes 2, 3 and 4
- It's important to remember to get your net to train nicely and overfit first, then regularize.
-




Layers were (512,256,128), the score was .428
Dropout(.15) -> Dense(n_in, l1, activation='tanh') -> BatchNormalization((l1,)) -> Dropout(.5) -> Dense(l1, l2) -> PReLU((l2,)) -> BatchNormalization((l2,)) -> Dropout(.3) -> Dense(l2, l3) -> PReLU((l3,)) -> BatchNormalization((l3,)) -> Dropout(.1) -> Dense(l3, n_out) -> Activation('softmax')
sgd = SGD(lr=0.004, decay=1e-7, momentum=0.99, nesterov=True)

Average several NNs

public score of 0.42399 combined with XGB
InputLayer (94) - DropoutLayer (94) - DenseLayer (1024) - DenseLayer (512) - DenseLayer (9)
dropout_p = 0.2
update=adagrad
update_learning_rate = linear decay from 0.02 to 0.001
eval_size = 0.2
max_epochs=100 + early stopping with patience = 7
output_nonlinearity=softmax

just removing stuff from the training set that had an individual logloss less than 2-3% of the average logloss, and iterating that 16 times or so. Each training sequence can be quite short, only 20 epochs or so

nolearn + lasagne wrapped with CallibratedClassifierCV (around 0.43)

I finally finished parameter and structure tuning for lasagne neural network, and get a single lasagne model to achieve 0.426 on LB

Pseudo-labeling didn't work at https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14297/share-your-models/79345#post79345

I've seen one person use dropout only after input layer, and another using it after every hidden layer (like I do) (good score) https://github.com/ahara/kaggle_otto/blob/master/otto/model/model_09_nn_adagrad/nn_adagrad.py
He also had momentum (epsilon) as ~0 (seems like an optimized value)

1000x700x600 / 1100x800x600 / 1300x1000x800  with dropout rates .20-.25 https://github.com/Kunstmord/kaggle-otto/blob/master/final.rst

Cloud computing (AWS?) spot instances (the cheap, leftover part that can be terminated at any time). Can get a trial for 10ish days at Google Compute or Microsoft Azure


