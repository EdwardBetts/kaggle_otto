
read until May 9th: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/12882/guess-this-is-really-the-time-to-try-out-neural-nets?page=5
read until May 9th: feature: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14092/feature-generation-for-dataset-ideas
read until May 9th: NN https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13632/achieve-0-48-in-5-min-with-a-deep-net-feat-batchnorm-prelu
read until May 9th: feature: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14092/feature-generation-for-dataset-ideas
read until May 9th: difficult cls: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13768/how-to-classify-the-class-2-3-4-better
long tutorial: http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/


#todo: investigate overfitting better!
#todo: predictions from several nets into one

Maybe make several NNs and feed the probabilities into a new NN
The key point of my method is that I tuned NN hyper parameters for an averaging model rather than for a single model.
BatchNorm and PReLU

Kinda done:
- An overfitting net can generally be made to perform better by using more training data.
- If your unregularized net does not overfit, you should probably make it larger.
- If I can store just the weights and biasses, I can start the network from those points instead of reloading everything.
- Dealing with class imbalance: undersampling, class weight, and rescaling probabilities
-

Maybe later:
- Really hard time to distinguish classes 2, 3 and 4
- It's important to remember to get your net to train nicely and overfit first, then regularize.
-

