
read until May 9th: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/12882/guess-this-is-really-the-time-to-try-out-neural-nets?page=5
read until May 9th: feature: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14092/feature-generation-for-dataset-ideas
read until May 9th: NN https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13632/achieve-0-48-in-5-min-with-a-deep-net-feat-batchnorm-prelu
read until May 9th: difficult cls: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13768/how-to-classify-the-class-2-3-4-better
long tutorial: http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/

getting unreasonably low scores must mean overfitting, which I think cannot be caused by weight decay... somehow information is leaking?
could it be a shared memory problem? that would really suck... but unlikely if they're different processes
note that the error occurs in network and in cross validation, so it's not a NN bug but a data leak bug
lasagne/nolearn is close to cross validation so that's probably okay

#todo: investigate overfitting better!
#todo: predictions from several nets into one
#todo: I could train ~10/25 epochs at a time, then change the weight decay in between and filter out easy samples
    #todo ... BUT I SHOULD NOT DO THAT BECAUSE IT'LL TAKE A LONG TIME
Maybe the network remembers the current epoch between calling .fit() twice
Note that the end-of-iteration handlers will call every epoch_steps
Well that was a great idea but I don't keep epochs since I have to re-initialize the network to reset the objective

Winning features:
-Feature 1: Distances to nearest neighbours of each classes
-Feature 2: Sum of distances of 2 nearest neighbours of each classes
-Feature 3: Sum of distances of 4 nearest neighbours of each classes
-Feature 4: Distances to nearest neighbours of each classes in TFIDF space
-Feature 5: Distances to nearest neighbours of each classed in T-SNE space (3 dimensions)
-Feature 6: Clustering features of original dataset
-Feature 7: Number of non-zeros elements in each row
It is the output of standard KNeighborsClassifier from Scikit-Learn with weights 'uniform'. But you can also set weights to 'distance', it will improves the score! ;)

top team (?): But he uses 20% data for level 1 and the reset for level 2.

Maybe make several NNs and feed the probabilities into a new NN
The key point of my method is that I tuned NN hyper parameters for an averaging model rather than for a single model.
BatchNorm and PReLU

Kinda done:
- An overfitting net can generally be made to perform better by using more training data.
- If your unregularized net does not overfit, you should probably make it larger.
- If I can store just the weights and biasses, I can start the network from those points instead of reloading everything.
- Dealing with class imbalance: undersampling, class weight, and rescaling probabilities
-

Maybe later:
- Really hard time to distinguish classes 2, 3 and 4
- It's important to remember to get your net to train nicely and overfit first, then regularize.
-


good results: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13851/lasagne-with-2-hidden-layers/75923#post75923
50 bags can apparently help - the trick with bagging is to use different sample of train data each time, then just take e.g. average
bagging automatically: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html

permalink to a good performing bagged NN: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14295/41599-via-tsne-meta-bagging/79084#post79084

Is there a spearmint version of sklearn Grid/Random SearchCV? The lite version seems reasonable: https://github.com/JasperSnoek/spearmint

save each optimized network, possibly with loss in the name



For NN with 0.44 single: No features scaling was used, with the raw data itself would be good.

Layers were (512,256,128), the score was .428
Dropout(.15) -> Dense(n_in, l1, activation='tanh') -> BatchNormalization((l1,)) -> Dropout(.5) -> Dense(l1, l2) -> PReLU((l2,)) -> BatchNormalization((l2,)) -> Dropout(.3) -> Dense(l2, l3) -> PReLU((l3,)) -> BatchNormalization((l3,)) -> Dropout(.1) -> Dense(l3, n_out) -> Activation('softmax')
sgd = SGD(lr=0.004, decay=1e-7, momentum=0.99, nesterov=True)

Average several NNs

public score of 0.42399 combined with XGB
InputLayer (94) - DropoutLayer (94) - DenseLayer (1024) - DenseLayer (512) - DenseLayer (9)
dropout_p = 0.2
update=adagrad
update_learning_rate = linear decay from 0.02 to 0.001
eval_size = 0.2
max_epochs=100 + early stopping with patience = 7
output_nonlinearity=softmax

just removing stuff from the training set that had an individual logloss less than 2-3% of the average logloss, and iterating that 16 times or so. Each training sequence can be quite short, only 20 epochs or so

nolearn + lasagne wrapped with CallibratedClassifierCV (around 0.43)

I finally finished parameter and structure tuning for lasagne neural network, and get a single lasagne model to achieve 0.426 on LB

Pseudo-labeling didn't work at https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14297/share-your-models/79345#post79345

I've seen one person use dropout only after input layer, and another using it after every hidden layer (like I do) (good score) https://github.com/ahara/kaggle_otto/blob/master/otto/model/model_09_nn_adagrad/nn_adagrad.py
He also had momentum (epsilon) as ~0 (seems like an optimized value)

1000x700x600 / 1100x800x600 / 1300x1000x800  with dropout rates .20-.25 https://github.com/Kunstmord/kaggle-otto/blob/master/final.rst

Cloud computing (AWS?) spot instances (the cheap, leftover part that can be terminated at any time). Can get a trial for 10ish days at Google Compute or Microsoft Azure

-log((1-p)/p)  http://en.wikipedia.org/wiki/Logit

It's extremely easy to overfit when blending, particularly if you're using the same data to determine the blending weights as you are when fitting the original model.

One architecture I ended up getting reasonable results (LB=0.446) with was:
    Input -> 0.13 Dropout -> 2500 -> 0.77 Dropout -> 1300 -> 0.35 Dropout -> 40 -> 0.1 Dropout -> Output (AdaGrad, 600 epochs, batch=1024, linear decreasing learning rate starting at 0.04, ending at 0.005)
However, I also had networks with (LB=0.441):
    Input -> 0.15 Drop -> 2000 -> 0.7 Drop -> 1200 -> 0.4 Drop -> 40 -> Output (AdaGrad, 400 epochs, batch = 512, learning rate from 0.04 to 0.001)
or even (LB=0.463):
    Input -> 0.15 Drop -> 180 -> 0.1 GaussianNoise -> 0.1 Drop -> 180 -> 0.05 Drop -> 180 -> 0.05 Drop -> 140 -> 0.05 Drop -> 140 -> 0.05 Drop -> 140 -> 0.1 Drop -> 140 -> 0.1 GaussianNoise -> 160 -> Output (AdaGrad, 100 epochs, batch=512, rate = 0.03 to 0.005)

Boosted Neural Network: remove the high confidence training samples every 25 epochs: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14296/competition-write-up-optimistically-convergent

Another NN layout with okay score and little work: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14303/0-416-in-just-a-few-lines-of-code

Most people seem to tune NN by hand watching learning curves. So I might want to save those things.

smart ensembling and having multiple different biases seemed to be the most important things for this competition

...neural network ensemble algorithm had completely outpaced it

in the last few days of the competition, we went back and stuck Scikit-Learn's CalibratedClassifierCV on things. It seemed to make a huge difference in the local CV of the neural networks to do 10-fold isotonic calibration. Stuff that was getting 0.443 on local CV was getting down to 0.435.

The other side of it is if the ratio is above 1 but I'm not seeing improvements in either score, then that generally indicates that I have too much dropout (but the architecture might be broad enough).

Optimistically convergent:
-Layer Sizes: [93, 400, 400, 9]
 Dropout: [.1, .4, .4]
 Nesterov Momentum: 0.9 -
 0.999
 Learning Rate: 0.008 - 0.001
 Max Epochs: 400
-Layer Sizes: [93, 512, 512, 512,
 9]
 Dropout: [.2, .4, .4, .4]
 Nesterov Momentum: .9 - .999
 Learning Rate: .017 - 1e-6
 Max Epochs: 1600

RandomSearch: center on the current best point




