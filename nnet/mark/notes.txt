
read until May 9th: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/12882/guess-this-is-really-the-time-to-try-out-neural-nets?page=5
read until May 9th: feature: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14092/feature-generation-for-dataset-ideas
read until May 9th: NN https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13632/achieve-0-48-in-5-min-with-a-deep-net-feat-batchnorm-prelu
read until May 9th: feature: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14092/feature-generation-for-dataset-ideas
read until May 9th: difficult cls: https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13768/how-to-classify-the-class-2-3-4-better
long tutorial: http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/


#todo: BatchNorm and PReLU
#todo: predictions from several nets into one
#todo: combine more variables in the features (most are just the original variable now)

That's a single network with dropout. CV is 0.445 and LB is 0.448. I'll be happy to post the code afterwards, but currently its nothing special, just parameter tuning so far.
My problem is that I was able to get 0.43702 with nolearn/lasagne - https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13799/what-models-may-give-score-under-0-5
simple linear combination of NN (.46107) and XGBoost (.43941) puts me at .4259.
Maybe make several NNs and feed the probabilities into a new NN
The key point of my method is that I tuned NN hyper parameters for an averaging model rather than for a single model.


Kinda done:
- An overfitting net can generally be made to perform better by using more training data.
- If your unregularized net does not overfit, you should probably make it larger.
- If I can store just the weights and biasses, I can start the network from those points instead of reloading everything.
- Dealing with class imbalance: undersampling, class weight, and rescaling probabilities
-

Maybe later:
- Really hard time to distinguish classes 2, 3 and 4
- It's important to remember to get your net to train nicely and overfit first, then regularize.
-

