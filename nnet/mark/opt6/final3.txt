nohup: ignoring input
Couldn't import dot_parser, loading of dot files will not be possible.
loaded transformed NN train and test data from cache in "/tmp" with key ""
Fitting 1 folds for each of 20 candidates, totalling 20 fits
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
INFO (theano.gof.compilelock): Waiting for existing lock by process '6152' (I am process '21220')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '21214')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
WARNING (theano.gof.cmodule): Deleting (broken cache directory [EOF]): /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/tmp4E90XF
INFO (theano.gof.compilelock): Waiting for existing lock by process '21212' (I am process '21206')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '21212')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
INFO (theano.gof.compilelock): Waiting for existing lock by process '43582' (I am process '21212')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '43600' (I am process '21212')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '21212')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '21204')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/home/mverleg/mlip2/env/src/lasagne/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '21208')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '551' (I am process '21208')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '21208')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '548' (I am process '21208')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '21208')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '6141' (I am process '21208')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '6146' (I am process '21208')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '43609' (I am process '21208')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '6138' (I am process '21208')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '21208')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/lock_dir
!! Could not restore the network to the state of minimum; only the final network has been saved.
!! Could not restore the network to the state of minimum; only the final network has been saved.
!! Could not restore the network to the state of minimum; only the final network has been saved.
!! Could not restore the network to the state of minimum; only the final network has been saved.
!! Could not restore the network to the state of minimum; only the final network has been saved.
!! Could not restore the network to the state of minimum; only the final network has been saved.
!! Could not restore the network to the state of minimum; only the final network has been saved.
[Parallel(n_jobs=31)]: Done   1 out of  20 | elapsed: 217.7min remaining: 4135.6min
!! Could not restore the network to the state of minimum; only the final network has been saved.
!! Could not restore the network to the state of minimum; only the final network has been saved.
!! Could not restore the network to the state of minimum; only the final network has been saved.
[Parallel(n_jobs=31)]: Done  10 out of  20 | elapsed: 257.0min remaining: 257.0min
!! Could not restore the network to the state of minimum; only the final network has been saved.
!! Could not restore the network to the state of minimum; only the final network has been saved.
adding all extra row features
adding 40 features for classes (2, 3)
adding 40 features for classes (2, 3, 4)
adding 63 features for classes (1, 9)
applying log transform to 49502x253 data
initializing network final_final3_7490 400x300x300
parameters: adaptive_weight_decay = False, auto_stopping = True, batch_size = 128, dense1_init = glorot_uniform, dense1_nonlinearity = rectify, dense1_size = 400, dense2_init = glorot_uniform, dense2_nonlinearity = rectify, dense2_size = 300, dense3_init = glorot_uniform, dense3_nonlinearity = rectify, dense3_size = 300, dropout0_rate = 0, dropout1_rate = 0.0506272323031, dropout2_rate = 0.418702329624, dropout3_rate = 0.670432595338, epoch_steps = None, learning_rate = 0.000117257704919, learning_rate_scaling = 500, max_epochs = 800, momentum = 0.98, momentum_scaling = 10, name = final_final3_7490, output_nonlinearity = softmax, save_snapshots_stepsize = None, weight_decay = 3e-05
  input             	(None, 253)         	produces     253 outputs
  dense1            	(None, 400)         	produces     400 outputs
  dropout1          	(None, 400)         	produces     400 outputs
  dense2            	(None, 300)         	produces     300 outputs
  dropout2          	(None, 300)         	produces     300 outputs
  dense3            	(None, 300)         	produces     300 outputs
  dropout3          	(None, 300)         	produces     300 outputs
  output            	(None, 9)           	produces       9 outputs
adding all extra row features
adding 40 features for classes (2, 3)
adding 40 features for classes (2, 3, 4)
adding 63 features for classes (1, 9)
applying log transform to 49502x253 data
initializiTraceback (most recent call last):
  File "nnet/optimize/final3.py", line 87, in <module>
    opt.fit(train, labels)
  File "/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/sklearn/grid_search.py", line 898, in fit
    return self._fit(X, y, sampled_params)
  File "/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/sklearn/grid_search.py", line 505, in _fit
    for parameters in parameter_iterable
  File "/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py", line 666, in __call__
    self.retrieve()
  File "/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py", line 549, in retrieve
    raise exception_type(report)
sklearn.externals.joblib.my_exceptions.JoblibIOError: JoblibIOError
___________________________________________________________________________
Multiprocessing exception:
    ...........................................................................
/home/mverleg/mlip2/nnet/optimize/final3.py in <module>()
     82 	random_state = random,
     83 	verbose = bool(VERBOSITY),
     84 	error_score = 'raise',
     85 )
     86 
---> 87 opt.fit(train, labels)
     88 
     89 print '> saving results for top score {0:.4f} (adding rescaling to priors):'.format(-opt.best_score_), opt.best_params_
     90 try:
     91 	with open(join(LOGS_DIR, 'random_search_{0:.4f}.json'.format(-opt.best_score_)), 'w+') as fh:

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/sklearn/grid_search.py in fit(self=RandomizedSearchCV(cv=ShuffleSplit(61878, n_iter...False, needs_proba=True),
          verbose=True), X=array([[ 1,  0,  0, ...,  0,  0,  0],
       [ 0...    [ 0,  0,  0, ...,  0,  2,  0]], dtype=uint16), y=array([1, 1, 1, ..., 9, 9, 9], dtype=uint16))
    893 
    894         """
    895         sampled_params = ParameterSampler(self.param_distributions,
    896                                           self.n_iter,
    897                                           random_state=self.random_state)
--> 898         return self._fit(X, y, sampled_params)
        self._fit = <bound method RandomizedSearchCV._fit of Randomi...alse, needs_proba=True),
          verbose=True)>
        X = array([[ 1,  0,  0, ...,  0,  0,  0],
       [ 0...    [ 0,  0,  0, ...,  0,  2,  0]], dtype=uint16)
        y = array([1, 1, 1, ..., 9, 9, 9], dtype=uint16)
        sampled_params = <sklearn.grid_search.ParameterSampler object>
    899 
    900 
    901 
    902 

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/sklearn/grid_search.py in _fit(self=RandomizedSearchCV(cv=ShuffleSplit(61878, n_iter...False, needs_proba=True),
          verbose=True), X=array([[ 1,  0,  0, ...,  0,  0,  0],
       [ 0...    [ 0,  0,  0, ...,  0,  2,  0]], dtype=uint16), y=array([1, 1, 1, ..., 9, 9, 9], dtype=uint16), parameter_iterable=<sklearn.grid_search.ParameterSampler object>)
    500         )(
    501             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,
    502                                     train, test, self.verbose, parameters,
    503                                     self.fit_params, return_parameters=True,
    504                                     error_score=self.error_score)
--> 505                 for parameters in parameter_iterable
        parameters = undefined
        parameter_iterable = <sklearn.grid_search.ParameterSampler object>
    506                 for train, test in cv)
    507 
    508         # Out is a list of triplet: score, estimator, n_test_samples
    509         n_fits = len(out)

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=31), iterable=<itertools.islice object>)
    661             if pre_dispatch == "all" or n_jobs == 1:
    662                 # The iterable was consumed all at once by the above for loop.
    663                 # No need to wait for async callbacks to trigger to
    664                 # consumption.
    665                 self._iterating = False
--> 666             self.retrieve()
        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=31)>
    667             # Make sure that we get a last message telling us we are done
    668             elapsed_time = time.time() - self._start_time
    669             self._print('Done %3i out of %3i | elapsed: %s finished',
    670                         (len(self._output),

    ---------------------------------------------------------------------------
    Sub-process traceback:
    ---------------------------------------------------------------------------
    IOError                                            Thu Jun 18 11:16:33 2015
PID: 21208                 Python 2.7.3: /home/mverleg/mlip2/env/bin/python
...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/sklearn/cross_validation.pyc in _fit_and_score(estimator=Pipeline(steps=[('row', PositiveSparseRowFeature...napshots_stepsize=None,
   weight_decay=1e-05))]), X=array([[ 1,  0,  0, ...,  0,  0,  0],
       [ 0...    [ 0,  0,  0, ...,  0,  2,  0]], dtype=uint16), y=array([1, 1, 1, ..., 9, 9, 9], dtype=uint16), scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True), train=array([41396, 45752, 11745, ..., 33858,  5780, 10527]), test=array([47151, 51142, 46501, ..., 55539, 45032, 44434]), verbose=True, parameters={'nn__dropout1_rate': 0.056495110939264376, 'nn__dropout2_rate': 0.40561962723257516, 'nn__dropout3_rate': 0.4084918920727297, 'nn__learning_rate': 9.264146873356604e-05, 'nn__name': 'final_final3_7411', 'nn__weight_decay': 1e-05}, fit_params={'nn__random_sleep': 600}, return_train_score=False, return_parameters=True, error_score='raise')
   1454 
   1455     try:
   1456         if y_train is None:
   1457             estimator.fit(X_train, **fit_params)
   1458         else:
-> 1459             estimator.fit(X_train, y_train, **fit_params)
   1460 
   1461     except Exception as e:
   1462         if error_score == 'raise':
   1463             raise

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/sklearn/pipeline.pyc in fit(self=Pipeline(steps=[('row', PositiveSparseRowFeature...napshots_stepsize=None,
   weight_decay=1e-05))]), X=array([[ 6,  2,  1, ...,  0,  1, 40],
       [ 0...    [ 0,  0,  0, ...,  0,  0,  0]], dtype=uint16), y=array([6, 7, 2, ..., 6, 2, 2], dtype=uint16), **fit_params={'random_sleep': 600})
    136         y : iterable, default=None
    137             Training targets. Must fulfill label requirements for all steps of
    138             the pipeline.
    139         """
    140         Xt, fit_params = self._pre_transform(X, y, **fit_params)
--> 141         self.steps[-1][-1].fit(Xt, y, **fit_params)
    142         return self
    143 
    144     def fit_transform(self, X, y=None, **fit_params):
    145         """Fit all the transforms one after the other and transform the

...........................................................................
/home/mverleg/mlip2/nnet/scikit.pyc in fit(self=NNet(adaptive_weight_decay=False, auto_stopping=...e_snapshots_stepsize=None,
   weight_decay=1e-05), X=array([[ 1.44389224,  0.83412641,  0.49814296, ....        0.        ,  0.        ]], dtype=float32), y=array([6, 7, 2, ..., 6, 2, 2], dtype=uint16), random_sleep=600)
    293 
    294 	def fit(self, X, y, random_sleep = None):
    295 		if random_sleep:
    296 			sleep(random_sleep * random())  # this is to prevent compiler lock problems
    297 		labels = y - y.min()
--> 298 		self.init_net(feature_count = X.shape[1], class_count = labels.max() + 1)
    299 		net = self.net.fit(X, labels)
    300 		self.save()
    301 		return net
    302 

...........................................................................
/home/mverleg/mlip2/nnet/scikit.pyc in init_net(self=NNet(adaptive_weight_decay=False, auto_stopping=...e_snapshots_stepsize=None,
   weight_decay=1e-05), feature_count=253, class_count=9, verbosity=True)
    245 
    246 			**self.params
    247 		)
    248 		self.net.parent = self
    249 
--> 250 		self.net.initialize()
    251 
    252 		return self.net
    253 
    254 	def get_params(self, deep = True):

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/nolearn/lasagne/base.pyc in initialize(self=NeuralNet(X_tensor_type=<function matrix at 0x27...ue,
     y_tensor_type=TensorType(int32, vector)))
    161             self.X_tensor_type = types[len(first_layer.shape)]
    162 
    163         iter_funcs = self._create_iter_funcs(
    164             self.layers_, self.objective, self.update,
    165             self.X_tensor_type,
--> 166             self.y_tensor_type,
    167             )
    168         self.train_iter_, self.eval_iter_, self.predict_iter_ = iter_funcs
    169         self._initialized = True
    170 

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/nolearn/lasagne/base.pyc in _create_iter_funcs(self=NeuralNet(X_tensor_type=<function matrix at 0x27...ue,
     y_tensor_type=TensorType(int32, vector)), layers=OrderedDict([('input', <lasagne.layers.input.Inp...e.layers.dense.DenseLayer object at 0x5fb6d50>)]), objective=<functools.partial object>, update=<function nesterov_momentum>, input_type=<function matrix>, output_type=TensorType(int32, vector))
    259             inputs=[theano.Param(X_batch), theano.Param(y_batch)],
    260             outputs=[loss_train],
    261             updates=updates,
    262             givens={
    263                 X: X_batch,
--> 264                 y: y_batch,
    265                 },
    266             )
    267         eval_iter = theano.function(
    268             inputs=[theano.Param(X_batch), theano.Param(y_batch)],

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/compile/function.pyc in function(inputs=[<theano.compile.pfunc.Param object>, <theano.compile.pfunc.Param object>], outputs=[Elemwise{add,no_inplace}.0], mode=None, updates=OrderedDict([(dense1.W, Elemwise{add,no_inplace}...(float64, vector)>, Elemwise{sub,no_inplace}.0)]), givens={x: x_batch, y: y_batch}, no_default_updates=False, accept_inplace=False, name='/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/nolearn/lasagne/base.py:264', rebuild_strict=True, allow_input_downcast=None, profile=None, on_unused_input=None)
    261                 no_default_updates=no_default_updates,
    262                 accept_inplace=accept_inplace, name=name,
    263                 rebuild_strict=rebuild_strict,
    264                 allow_input_downcast=allow_input_downcast,
    265                 on_unused_input=on_unused_input,
--> 266                 profile=profile)
    267     # We need to add the flag check_aliased inputs if we have any mutable or
    268     # borrowed used defined inputs
    269     fn._check_for_aliased_inputs = check_for_aliased_inputs
    270     return fn

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/compile/pfunc.pyc in pfunc(params=[<theano.compile.pfunc.Param object>, <theano.compile.pfunc.Param object>], outputs=[Elemwise{add,no_inplace}.0], mode=None, updates=OrderedDict([(dense1.W, Elemwise{add,no_inplace}...(float64, vector)>, Elemwise{sub,no_inplace}.0)]), givens={x: x_batch, y: y_batch}, no_default_updates=False, accept_inplace=False, name='/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/nolearn/lasagne/base.py:264', rebuild_strict=True, allow_input_downcast=None, profile=False, on_unused_input=None)
    506                     mutable=False, borrow=True, shared=True)
    507         inputs.append(si)
    508 
    509     return orig_function(inputs, cloned_outputs, mode,
    510             accept_inplace=accept_inplace, name=name, profile=profile,
--> 511             on_unused_input=on_unused_input)
    512 
    513 
    514 def _pfunc_param_to_in(param, strict=False, allow_downcast=None):
    515     if isinstance(param, Constant):

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/compile/function_module.pyc in orig_function(inputs=[In(x_batch), In(y_batch), In(dense1.W -> Elemwise{add,no_inplace}.0), In(dense1.b -> Elemwise{add,no_inplace}.0), In(<TensorType(int32, matrix)> -> mrg_uniform{TensorType(float64, matrix),no_inplace}.0), In(dense2.W -> Elemwise{add,no_inplace}.0), In(dense2.b -> Elemwise{add,no_inplace}.0), In(<TensorType(int32, matrix)> -> mrg_uniform{TensorType(float64, matrix),no_inplace}.0), In(dense3.W -> Elemwise{add,no_inplace}.0), In(dense3.b -> Elemwise{add,no_inplace}.0), In(<TensorType(int32, matrix)> -> mrg_uniform{TensorType(float64, matrix),no_inplace}.0), In(output.W -> Elemwise{add,no_inplace}.0), In(output.b -> Elemwise{add,no_inplace}.0), In(weight_decay), In(<TensorType(float64, scalar)>), In(<TensorType(float64, matrix)> -> Elemwise{sub,no_inplace}.0), In(<TensorType(float32, scalar)>), In(<TensorType(float64, vector)> -> Elemwise{sub,no_inplace}.0), In(<TensorType(float64, matrix)> -> Elemwise{sub,no_inplace}.0), In(<TensorType(float64, vector)> -> Elemwise{sub,no_inplace}.0), ...], outputs=[Out(Elemwise{add,no_inplace}.0,False)], mode=<theano.compile.mode.Mode object>, accept_inplace=False, name='/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/nolearn/lasagne/base.py:264', profile=False, on_unused_input=None)
   1461                    outputs,
   1462                    mode,
   1463                    accept_inplace=accept_inplace,
   1464                    profile=profile,
   1465                    on_unused_input=on_unused_input).create(
-> 1466                        defaults)
   1467 
   1468     t2 = time.time()
   1469     if profile:
   1470         profile.compile_time += t2 - t1

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/compile/function_module.pyc in create(self=<theano.compile.function_module.FunctionMaker object>, input_storage=[None, None, <array([[-0.07512851,  0.05216339, -0.02579317, ...-0.06589605,
         0.02580125,  0.04872444]])>, <array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., ....,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])>, <array([[     12345,      12345,      12345,    ...,  184506923,
        2110382001]], dtype=int32)>, <array([[ 0.05147709, -0.09112379, -0.06438748, ... 0.01125079,
         0.05380207, -0.08332789]])>, <array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., ....,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])>, <array([[ 336690377,  597094797, 1245771585,   8...,  744179834,
         110486429]], dtype=int32)>, <array([[-0.08851689,  0.08916053, -0.00042248, ... 0.05033764,
        -0.03000036, -0.04275862]])>, <array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., ....,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])>, <array([[ 502033783, 1322587635, 1964121530, 194..., 1874665993,
        1874760456]], dtype=int32)>, <array([[ 0.00785605,  0.01532797,  0.09715899, ...-0.0352324 ,
         0.06781205, -0.02443479]])>, <array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])>, <array(9.999999747378752e-06, dtype=float32)>, <array(1e-05)>, <array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
   ....],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])>, <array(9.264147229259834e-05, dtype=float32)>, <array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., ....,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])>, <array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
   ....],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])>, <array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., ....,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])>, ...], trustme=False)
   1319         start_import_time = theano.gof.cmodule.import_time
   1320         limit_orig = theano.config.traceback.limit
   1321         try:
   1322             theano.config.traceback.limit = 0
   1323             _fn, _i, _o = self.linker.make_thunk(
-> 1324                 input_storage=input_storage_lists)
   1325         finally:
   1326             theano.config.traceback.limit = limit_orig
   1327 
   1328         end_linker = time.time()

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/gof/link.pyc in make_thunk(self=<theano.gof.vm.VM_Linker object>, input_storage=[[None], [None], [array([[-0.07512851,  0.05216339, -0.02579317, .... -0.06589605,
         0.02580125,  0.04872444]])], [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], [array([[     12345,      12345,      12345,     ...0,  184506923,
        2110382001]], dtype=int32)], [array([[ 0.05147709, -0.09112379, -0.06438748, ....  0.01125079,
         0.05380207, -0.08332789]])], [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], [array([[ 336690377,  597094797, 1245771585,   85...4,  744179834,
         110486429]], dtype=int32)], [array([[-0.08851689,  0.08916053, -0.00042248, ....  0.05033764,
        -0.03000036, -0.04275862]])], [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], [array([[ 502033783, 1322587635, 1964121530, 1949...3, 1874665993,
        1874760456]], dtype=int32)], [array([[ 0.00785605,  0.01532797,  0.09715899, .... -0.0352324 ,
         0.06781205, -0.02443479]])], [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], [array(9.999999747378752e-06, dtype=float32)], [array(1e-05)], [array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
    ...0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])], [array(9.264147229259834e-05, dtype=float32)], [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], [array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
    ...0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])], [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], ...], output_storage=None)
    514     thunk associated with each node.
    515     """
    516 
    517     def make_thunk(self, input_storage=None, output_storage=None):
    518         return self.make_all(input_storage=input_storage,
--> 519                              output_storage=output_storage)[:3]
    520 
    521     def make_all(self, input_storage, output_storage):
    522         # By convention, subclasses of LocalLinker should implement this function!
    523         #

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/gof/vm.pyc in make_all(self=<theano.gof.vm.VM_Linker object>, profiler=None, input_storage=[[None], [None], [array([[-0.07512851,  0.05216339, -0.02579317, .... -0.06589605,
         0.02580125,  0.04872444]])], [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], [array([[     12345,      12345,      12345,     ...0,  184506923,
        2110382001]], dtype=int32)], [array([[ 0.05147709, -0.09112379, -0.06438748, ....  0.01125079,
         0.05380207, -0.08332789]])], [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], [array([[ 336690377,  597094797, 1245771585,   85...4,  744179834,
         110486429]], dtype=int32)], [array([[-0.08851689,  0.08916053, -0.00042248, ....  0.05033764,
        -0.03000036, -0.04275862]])], [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], [array([[ 502033783, 1322587635, 1964121530, 1949...3, 1874665993,
        1874760456]], dtype=int32)], [array([[ 0.00785605,  0.01532797,  0.09715899, .... -0.0352324 ,
         0.06781205, -0.02443479]])], [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], [array(9.999999747378752e-06, dtype=float32)], [array(1e-05)], [array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
    ...0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])], [array(9.264147229259834e-05, dtype=float32)], [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], [array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
    ...0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])], [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], ...], output_storage=[[None], [None], [None], [None], [None], [None], [None], [None], [None], [None], [None], [None], [None], [None], [None], [None], [None], [None], [None], [None]])
    892         for node in order:
    893             try:
    894                 thunks.append(node.op.make_thunk(node,
    895                                                  storage_map,
    896                                                  compute_map,
--> 897                                                  no_recycling))
    898                 if not hasattr(thunks[-1], 'lazy'):
    899                     # We don't want all ops maker to think about lazy Ops.
    900                     # So if they didn't specify that its lazy or not, it isn't.
    901                     # If this member isn't present, it will crash later.

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/gof/op.pyc in make_thunk(self=<theano.tensor.elemwise.Elemwise object>, node=Elemwise{Composite{Cast{float64}(LT(i0, i1))}}[(...place}.1, TensorConstant{(1, 1) of ..4380372767}), storage_map={Elemwise{Composite{((i0 * i1) + (i2 - i3))}}[(0, 2)].0: [None], InplaceDimShuffle{x}.0: [None], Elemwise{Composite{(i0 * (Abs(i1) + i2 + i3) * i4)}}[(0, 2)].0: [None], Elemwise{Cast{float64}}.0: [None], InplaceDimShuffle{1,0}.0: [None], Elemwise{Composite{((i0 * i1) - i2)}}[(0, 1)].0: [None], InplaceDimShuffle{x,x}.0: [None], Sum{acc_dtype=float64}.0: [None], TensorConstant{0.529938960356}: [array(0.5299389603563717)], x_batch: [None], ...}, compute_map={Gemm{inplace}.0: [False], Elemwise{Composite{((i0 * i1) + (i2 - i3))}}[(0, 2)].0: [False], InplaceDimShuffle{x}.0: [False], Elemwise{Composite{(i0 * (Abs(i1) + i2 + i3) * i4)}}[(0, 2)].0: [False], Elemwise{Cast{float64}}.0: [False], InplaceDimShuffle{1,0}.0: [False], Elemwise{Composite{((i0 * i1) - i2)}}[(0, 1)].0: [False], InplaceDimShuffle{x,x}.0: [False], Sum{acc_dtype=float64}.0: [False], TensorConstant{0.529938960356}: [True], ...}, no_recycling=set([Elemwise{Composite{((i0 * i1) + (i2 - i3))}}[(0, 3)].0, Elemwise{Composite{((i0 * i1) + (i2 - i3))}}[(0, 2)].0, Gemm{inplace}.0, Elemwise{mul,no_inplace}.0, Elemwise{Composite{((i0 * i1) - i2)}}[(0, 1)].0, Elemwise{mul,no_inplace}.0, ...]))
    997                 theano.config.openmp = False
    998 
    999     def make_thunk(self, node, storage_map, compute_map, no_recycling):
   1000         self.update_self_openmp()
   1001         return super(OpenMPOp, self).make_thunk(node, storage_map,
-> 1002                                                 compute_map, no_recycling)
   1003 
   1004 
   1005 def simple_meth(tag):
   1006     def f(self):

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/gof/op.pyc in make_thunk(self=<theano.tensor.elemwise.Elemwise object>, node=Elemwise{Composite{Cast{float64}(LT(i0, i1))}}[(...place}.1, TensorConstant{(1, 1) of ..4380372767}), storage_map={Elemwise{Composite{((i0 * i1) + (i2 - i3))}}[(0, 2)].0: [None], InplaceDimShuffle{x}.0: [None], Elemwise{Composite{(i0 * (Abs(i1) + i2 + i3) * i4)}}[(0, 2)].0: [None], Elemwise{Cast{float64}}.0: [None], InplaceDimShuffle{1,0}.0: [None], Elemwise{Composite{((i0 * i1) - i2)}}[(0, 1)].0: [None], InplaceDimShuffle{x,x}.0: [None], Sum{acc_dtype=float64}.0: [None], TensorConstant{0.529938960356}: [array(0.5299389603563717)], x_batch: [None], ...}, compute_map={Gemm{inplace}.0: [False], Elemwise{Composite{((i0 * i1) + (i2 - i3))}}[(0, 2)].0: [False], InplaceDimShuffle{x}.0: [False], Elemwise{Composite{(i0 * (Abs(i1) + i2 + i3) * i4)}}[(0, 2)].0: [False], Elemwise{Cast{float64}}.0: [False], InplaceDimShuffle{1,0}.0: [False], Elemwise{Composite{((i0 * i1) - i2)}}[(0, 1)].0: [False], InplaceDimShuffle{x,x}.0: [False], Sum{acc_dtype=float64}.0: [False], TensorConstant{0.529938960356}: [True], ...}, no_recycling=set([Elemwise{Composite{((i0 * i1) + (i2 - i3))}}[(0, 3)].0, Elemwise{Composite{((i0 * i1) + (i2 - i3))}}[(0, 2)].0, Gemm{inplace}.0, Elemwise{mul,no_inplace}.0, Elemwise{Composite{((i0 * i1) - i2)}}[(0, 1)].0, Elemwise{mul,no_inplace}.0, ...]))
    734                 cl = theano.gof.cc.CLinker().accept(e,
    735                         no_recycling=e_no_recycling)
    736 
    737                 logger.debug('Trying CLinker.make_thunk')
    738                 outputs = cl.make_thunk(input_storage=node_input_storage,
--> 739                                         output_storage=node_output_storage)
    740                 fill_storage, node_input_filters, node_output_filters = outputs
    741 
    742                 def rval():
    743                     fill_storage()

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/gof/cc.pyc in make_thunk(self=<theano.gof.cc.CLinker object>, input_storage=[[None], [array([[ 0.59438037]])]], output_storage=[[None]], keep_lock=False)
   1068           first_output = ostor[0].data
   1069         """
   1070         init_tasks, tasks = self.get_init_tasks()
   1071         cthunk, in_storage, out_storage, error_storage = self.__compile__(
   1072             input_storage, output_storage,
-> 1073             keep_lock=keep_lock)
   1074 
   1075         res = _CThunk(cthunk, init_tasks, tasks, error_storage)
   1076         res.nodes = self.node_order
   1077         return res, in_storage, out_storage

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/gof/cc.pyc in __compile__(self=<theano.gof.cc.CLinker object>, input_storage=([None], [array([[ 0.59438037]])]), output_storage=([None],), keep_lock=False)
   1010         input_storage = tuple(input_storage)
   1011         output_storage = tuple(output_storage)
   1012         thunk = self.cthunk_factory(error_storage,
   1013                                     input_storage,
   1014                                     output_storage,
-> 1015                                     keep_lock=keep_lock)
   1016         return (thunk,
   1017                 [link.Container(input, storage) for input, storage in
   1018                  izip(self.fgraph.inputs, input_storage)],
   1019                 [link.Container(output, storage, True) for output, storage in

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/gof/cc.pyc in cthunk_factory(self=<theano.gof.cc.CLinker object>, error_storage=[None, None, None], in_storage=([None], [array([[ 0.59438037]])]), out_storage=([None],), keep_lock=False)
   1437         if key is None:
   1438             # If we can't get a key, then forget the cache mechanism.
   1439             module = self.compile_cmodule()
   1440         else:
   1441             module = get_module_cache().module_from_key(
-> 1442                 key=key, lnk=self, keep_lock=keep_lock)
   1443 
   1444         vars = self.inputs + self.outputs + self.orphans
   1445         # List of indices that should be ignored when passing the arguments
   1446         # (basically, everything that the previous call to uniq eliminated)

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/gof/cmodule.pyc in module_from_key(self=<theano.gof.cmodule.ModuleCache object>, key=(((11, (3, (4,), (3, 4)), (13, '1.9.2'), (13, '1.9.2'), (13, '1.9.2'), ('openmp', False)), (11, 13, '1.9.2'), (11, 13, '1.9.2'), (11, 13, '1.9.2')), ('CLinker.cmodule_key', ('--param', '--param', '--param', '-D NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION', '-O3', '-Wno-unused-label', '-Wno-unused-variable', '-Wno-write-strings', '-fPIC', '-fno-math-errno', '-m64', '-maes', '-march=corei7-avx', '-mavx', '-mcx16', '-mno-abm', '-mno-bmi', '-mno-fma', '-mno-fma4', '-mno-lwp', ...), (), (), 'NPY_ABI_VERSION=0x1000009', u'c_compiler_str=/usr/bin/g++ 4.6', 'md5:8e4d09db1d568f01d6e4a755d07f3653', (<theano.tensor.elemwise.Elemwise object>, ((TensorType(float64, matrix), ((...), False)), (TensorType(float64, (True, True)), ((...), False))), (1, (False,))))), lnk=<theano.gof.cc.CLinker object>, keep_lock=False)
   1049         lock_taken = False
   1050 
   1051         src_code = lnk.get_src_code()
   1052         # Is the source code already in the cache?
   1053         module_hash = get_module_hash(src_code, key)
-> 1054         module = self._get_from_hash(module_hash, key, keep_lock=keep_lock)
   1055         if module is not None:
   1056             return module
   1057 
   1058         with compilelock.lock_ctx(keep_lock=keep_lock):

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/gof/cmodule.pyc in _get_from_hash(self=<theano.gof.cmodule.ModuleCache object>, module_hash='803e499470aa38ff97f93dc410ae3292', key=(((11, (3, (4,), (3, 4)), (13, '1.9.2'), (13, '1.9.2'), (13, '1.9.2'), ('openmp', False)), (11, 13, '1.9.2'), (11, 13, '1.9.2'), (11, 13, '1.9.2')), ('CLinker.cmodule_key', ('--param', '--param', '--param', '-D NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION', '-O3', '-Wno-unused-label', '-Wno-unused-variable', '-Wno-write-strings', '-fPIC', '-fno-math-errno', '-m64', '-maes', '-march=corei7-avx', '-mavx', '-mcx16', '-mno-abm', '-mno-bmi', '-mno-fma', '-mno-fma4', '-mno-lwp', ...), (), (), 'NPY_ABI_VERSION=0x1000009', u'c_compiler_str=/usr/bin/g++ 4.6', 'md5:8e4d09db1d568f01d6e4a755d07f3653', (<theano.tensor.elemwise.Elemwise object>, ((TensorType(float64, matrix), ((...), False)), (TensorType(float64, (True, True)), ((...), False))), (1, (False,))))), keep_lock=False)
    952         if module_hash in self.module_hash_to_key_data:
    953             key_data = self.module_hash_to_key_data[module_hash]
    954             module = self._get_from_key(None, key_data)
    955             with compilelock.lock_ctx(keep_lock=keep_lock):
    956                 try:
--> 957                     key_data.add_key(key, save_pkl=bool(key[0]))
    958                     key_broken = False
    959                 except cPickle.PicklingError:
    960                     key_data.remove_key(key)
    961                     key_broken = True

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/gof/cmodule.pyc in add_key(self=<theano.gof.cmodule.KeyData object>, key=(((11, (3, (4,), (3, 4)), (13, '1.9.2'), (13, '1.9.2'), (13, '1.9.2'), ('openmp', False)), (11, 13, '1.9.2'), (11, 13, '1.9.2'), (11, 13, '1.9.2')), ('CLinker.cmodule_key', ('--param', '--param', '--param', '-D NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION', '-O3', '-Wno-unused-label', '-Wno-unused-variable', '-Wno-write-strings', '-fPIC', '-fno-math-errno', '-m64', '-maes', '-march=corei7-avx', '-mavx', '-mcx16', '-mno-abm', '-mno-bmi', '-mno-fma', '-mno-fma4', '-mno-lwp', ...), (), (), 'NPY_ABI_VERSION=0x1000009', u'c_compiler_str=/usr/bin/g++ 4.6', 'md5:8e4d09db1d568f01d6e4a755d07f3653', (<theano.tensor.elemwise.Elemwise object>, ((TensorType(float64, matrix), ((...), False)), (TensorType(float64, (True, True)), ((...), False))), (1, (False,))))), save_pkl=True)
    462     def add_key(self, key, save_pkl=True):
    463         """Add a key to self.keys, and update pickled file if asked to."""
    464         assert key not in self.keys
    465         self.keys.add(key)
    466         if save_pkl:
--> 467             self.save_pkl()
    468 
    469     def remove_key(self, key, save_pkl=True):
    470         """Remove a key from self.keys, and update pickled file if asked to."""
    471         self.keys.remove(key)

...........................................................................
/home/mverleg/mlip2/env/local/lib/python2.7/site-packages/theano/gof/cmodule.pyc in save_pkl(self=<theano.gof.cmodule.KeyData object>)
    479         May raise a cPickle.PicklingError if such an exception is raised at
    480         pickle time (in which case a warning is also displayed).
    481         """
    482         # Note that writing in binary mode is important under Windows.
    483         try:
--> 484             with open(self.key_pkl, 'wb') as f:
    485                 cPickle.dump(self, f, protocol=cPickle.HIGHEST_PROTOCOL)
    486         except cPickle.PicklingError:
    487             _logger.warning("Cache leak due to unpickle-able key data %s",
    488                             self.keys)

IOError: [Errno 2] No such file or directory: '/home/mverleg/.theano/compiledir_Linux-3.2--generic-x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/tmpAGY__E/key.pkl'
___________________________________________________________________________
